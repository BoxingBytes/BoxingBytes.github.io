<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" href="/assets/css/style.css?v=c5d02b5abf9ae1b82c1a2be1ca00a668db55b240">
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<!-- Begin Jekyll SEO tag v2.8.0 -->
<title>My personal view of exploration in RL | BoxingBytes</title>
<meta name="generator" content="Jekyll v3.10.0" />
<meta property="og:title" content="My personal view of exploration in RL" />
<meta name="author" content="Aguirre Max" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Personal note: goal of this: explain very clearly &amp; intuitively how I view exploration currently, to be updated" />
<meta property="og:description" content="Personal note: goal of this: explain very clearly &amp; intuitively how I view exploration currently, to be updated" />
<link rel="canonical" href="http://localhost:4000/2025/08/24/exploration.html" />
<meta property="og:url" content="http://localhost:4000/2025/08/24/exploration.html" />
<meta property="og:site_name" content="BoxingBytes" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2025-08-24T00:00:00+02:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="My personal view of exploration in RL" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Aguirre Max"},"dateModified":"2025-08-24T00:00:00+02:00","datePublished":"2025-08-24T00:00:00+02:00","description":"Personal note: goal of this: explain very clearly &amp; intuitively how I view exploration currently, to be updated","headline":"My personal view of exploration in RL","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/2025/08/24/exploration.html"},"url":"http://localhost:4000/2025/08/24/exploration.html"}</script>
<!-- End Jekyll SEO tag -->

  </head>

  <body>

    <header>
      <div class="container">
        <a id="a-title" href="/">
          <h1>BoxingBytes</h1>
        </a>
        <h2>A blog about intelligence</h2>

        <section id="downloads">
          
          <a href="https://github.com/BoxingBytes/BoxingBytes.github.io" class="btn btn-github"><span class="icon"></span>View on GitHub</a>
        </section>
      </div>
    </header>

    <div class="container">
      <section id="main_content">
        <p>Personal note: goal of this: 
explain very clearly &amp; intuitively how I view exploration currently, to be updated</p>

<p>test video</p>
<video controls="" width="500">
  <source src="/assets/videos/breakout_lsd_right_skill.mp4" type="video/mp4" />
</video>

<p><img src="/assets/videos/breakout_left_skill.gif" alt="RL agent walking" /></p>

<p>“RL is a tradeoff between exploration and exploitation”. 
“You should explore more”. 
“Exploration is about going where you’ve never been”. 
“Exploration is about looking at state where you’re not sure about transitions”. 
You hear a lot about exploration in RL, and how it should be done. If you look at how it’s done in most algorithms, you will see some variant of epsilon greedy. In PPO you try to maximize some entropy term for instance. But exploration is much more subtle once you dive into it.</p>

<h1 id="exploration-as-copium">Exploration as copium</h1>

<p>Why do we want our agents to explore in the first place? To find a better solution? That’s not totally true. Here is a better list:</p>
<ul>
  <li>find another, maybe sub-optimal policy to solve the task, which may be usefull in slightly different scenarios</li>
  <li>understand the world better, to better one-shot downstream tasks later on</li>
  <li>higher asymptotic rewards</li>
  <li>sample efficiency in finding the optimal poilcy for a given task</li>
</ul>

<h1 id="the-three-main-approaches-to-exploration">The three main approaches to exploration</h1>

<p>There is curiosity-based. Simply put, it’s about visiting states where we were wrong about predicting transitions. Somehow it’s related to what we call world models, because this means we try to have an internal view of the environment’s dynamics, and we basically go back where our model was wrong.</p>

<p>Visitation-count style. Intuitively, you want to go everywhere to see what’s there.</p>

<p>Skill discovery. You are trying to discovery a sub-manifold of the state space where you can encode usefull “skills” for downstream tasks.</p>

<h1 id="making-your-agent-discover-skills-isnt-the-same-as-making-your-agent-discover-usefull-strategies">Making your agent discover skills isn’t the same as making your agent discover usefull strategies</h1>

<p>We see a lot of papers about skill-based exploration. Think DIAYN, LSD, CSD, METRA … (link?)
Already in itself it has a lot of issues and pitfall, which i’ll talk about later. But for now, let’s imagine you have a very powerfull algorithm which enables you to discover very clear distinct skills in an environment. This doesn’t mean you can solve any downstream tasks. Intuitively, people tend to go for Hierarchical Reinforcement Learning. This is where you freeze your policy which output skills.
\[\pi(a | s, z), \hat{a}\]
\(E = mc^2\)</p>

<h1 id="main-overview-of-an-exploration-algorithm">Main overview of an exploration algorithm</h1>

<p>In the end, from a very high-level overview, your algorithm should:</p>
<ul>
  <li>Explore its environments (we will see that it’s already much harder than one might think)</li>
  <li>Maximize its rewards</li>
</ul>

<h1 id="the-case-of-highly-non-stationary-environments">The case of highly non-stationary environments</h1>

<p>Even if your agent find an optimal policy, it might not stay optimal for very long. This is much more alike real life scenarios</p>

      </section>
    </div>
  </body>
</html>
